from typing import List, Tuple, Union

import torch.nn as nn
from torch import Tensor

from .layers.decoder import DecoderStage

DecoderOut = Union[Tuple[Tensor, ...], Tensor]


class DepthDecoder(nn.Module):
    """The full decoder architectue module.

    Note:
        This version is currently limited to 5 decoder stages.

    Args:
        layers (List[dict]): A list of configs for each decoder stage. These
            are unpacked and passed as kwargs to each stage.
    """
    def __init__(self, layers: List[dict]) -> None:

        super().__init__()

        self.layers = nn.ModuleList()

        for layer_config in layers:
            self.layers.append(DecoderStage(**layer_config))

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.xavier_uniform_(m.weight)

    def forward(self, left_image: Tensor, *feature_maps: Tensor,
                scale: float = 1) -> DecoderOut:
        """Estimate disparity given the left image and encoder feature maps.

        Args:
            left_image (Tensor): The original left image.
            *feature_maps: (Tensor): The feature maps from the encoder.
            scale (float, optional): A multiplier to scale the disparity from
                each decoder stage. Defaults to 1.

        Returns:
            Tuple[Tensor]: If in `.train()` mode, the model will return the
                disparity image from each stage. If in `.eval()` mode, the
                model will only return the final disparity.
        """
        f1, f2, f3, f4, x4 = feature_maps

        out5, skip5, _ = self.layers[0](x4, f4, x4, scale=scale)

        out4, skip4, disp4 = self.layers[1](out5, f3, skip5, scale=scale)
        out3, skip3, disp3 = self.layers[2](out4, f2, skip4, disp4, scale)
        out2, skip2, disp2 = self.layers[3](out3, f1, skip3, disp3, scale)

        _, _, disp1 = self.layers[4](out2, left_image, skip2, disp2, scale)

        if self.training:
            return disp1, disp2, disp3, disp4

        return disp1
